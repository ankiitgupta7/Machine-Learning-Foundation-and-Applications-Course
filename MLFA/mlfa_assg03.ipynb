{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlfa_assg03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQD4PwLEwf7P"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJdC4bTkFmMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3388ba4-cb16-4490-9e95-db052174fc01"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAVV8uFP0FbA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "b3cbac1f-c9cf-4e4c-fb88-a249d87c7ba9"
      },
      "source": [
        "dataset = pd.read_csv('traindata.csv')\n",
        "dataset = dataset.iloc[: , 0: 2]\n",
        "numfv = np.shape(dataset)[0] #num. of feature vectors for training\n",
        "dataset\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f6d7c2d0913a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'traindata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#num. of feature vectors for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lADPAQuUDh"
      },
      "source": [
        "**Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK52YII5ud5j"
      },
      "source": [
        "Part 1.1 Building a Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeOreiyzfrqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b741d2b5-2821-4855-91f7-c2bc93bb9d3a"
      },
      "source": [
        "all_stopwords = stopwords.words('english')\n",
        "#all_stopwords.remove('not') #considering it might play arole in classification\n",
        "#ps = PorterStemmer()\n",
        "sno = SnowballStemmer('english')\n",
        "\n",
        "corpus = [] #to contain the stemmed words without punctuations\n",
        "\n",
        "for i in range(0, 80): \n",
        "  news = re.sub('[^a-zA-Z]', ' ', dataset['text'][i]) #only alphabets are retained\n",
        "  news = news.lower() \n",
        "  news = news.split()\n",
        "  news = [sno.stem(word) for word in news if not word in set(all_stopwords)] \n",
        "  news = ' '.join(news) #join words again with a space\n",
        "  corpus.append(news)\n",
        "    \n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['outer space friend life extrem temperatur low pressur radiat quick degrad cell membran destroy dna',\n",
              " 'tenni origin name lawn tenni game two oppos player singl pair player doubl use taut strung racket hit ball specifi size weight bounc net rectangular court',\n",
              " 'one woman frequent flew southwest constant disappoint everi aspect compani oper fact becam known pen pal everi flight wrote complaint',\n",
              " 'decemb almost seven year mer outbreak novel coronavirus ncov surfac wuhan hubei region china',\n",
              " 'life form somehow find void soon die unless band togeth',\n",
              " 'small communiti new research show bacteria withstand harsh environ',\n",
              " 'ball deinococcus bacteria thin five sheet paper place outsid intern space station',\n",
              " 'stay three year microb heart ball surviv',\n",
              " 'group outer layer shield extrem space',\n",
              " 'known microb could surviv insid artifici meteorit first evid microb surviv long unprotect say margaret cramm',\n",
              " 'suggest life surviv space group say',\n",
              " 'cramm microbiologist univers calgari canada take part studi',\n",
              " 'say new find add weight worri human space travel could accident introduc life planet',\n",
              " 'akihiko yamagishi astrobiologist work institut space astronaut scienc tokyo japan',\n",
              " 'part team sent dri pellet deinococcus bacteriato space',\n",
              " 'radiat resist microb thrive extrem place earth stratospher',\n",
              " 'bacteria stuf small well metal plate nasa astronaut scott kelli affix plate exterior space station sampl sent back earth year',\n",
              " 'back home research moisten pellet also fed bacteria food wait',\n",
              " 'three year space bacteria micromet thick pellet make',\n",
              " 'dna studi suggest radiat fri genet materi',\n",
              " 'outer layer pellet micromet inch thick also dead',\n",
              " 'discolor ultraviolet radiat desicc dead cell shield inner microb hazard space',\n",
              " 'four everi microb larger pellet surviv yamagishi say',\n",
              " 'point award player team whenev oppon fail correct return ball within prescrib dimens court',\n",
              " 'organ tenni play accord rule sanction intern tenni feder itf world govern bodi sport',\n",
              " 'tenni origin known lawn tenni formal still britain play grass court victorian gentlemen ladi',\n",
              " 'play varieti surfac origin game trace th th centuri french handbal game call jeu de paum game palm deriv complex indoor racket ball game real tenni',\n",
              " 'ancient game still play limit degre usual call real tenni britain court tenni unit state royal tenni australia',\n",
              " 'period rapid growth particip spectat sport began late major championship open profession well amateur continu televis broadcast expand profession tournament circuit rise notabl player rivalri broaden appeal game',\n",
              " 'tenni enjoy player practic level skill top competit demand test shot make stamina rich stylist strateg varieti',\n",
              " 'origin garden parti game ladi whalebon corset starch petticoat men long white flannel evolv physic chess match player attack defend exploit angl technic weak stroke wide divers pace spin',\n",
              " 'itf nation associ constitut govern tenni worldwid overse intern competit davi cup feder cup tenni olymp game restor medal sport status game first time sinc',\n",
              " 'dimens tenni court feet metr singl feet metr doubl height net centr feet metr support side court post feet metr high place feet outsid court',\n",
              " 'tenni origin call lawn tenni grass court still use common court materi today clay call hard court place although unit state term refer hard surfac cement number cushion asphalt deriv synthet surfac',\n",
              " 'tenni ball consist pressur rubber core cover high qualiti cloth usual wool mix percent nylon',\n",
              " 'ball gradual go soft use tournament play chang regular interv agre upon offici depend upon factor court surfac ball must uniform outer surfac seam must stitchless',\n",
              " 'itf specifi ball must yellow white inch cm diamet ounc gram weight',\n",
              " 'oppon spin racket toss coin decid side servic winner may decid serv receiv servic first case oppon choos side may decid choic side case oppon may choos serv receiv servic first player serv altern game chang side everi odd number game',\n",
              " 'begin game behind right hand court server feet behind baselin strike ball diagon across net oppon right hand servic court',\n",
              " 'ball servic strike top net fall correct servic court let replay',\n",
              " 'server allow one miss fault either net outsid oppon servic court failur deliv correct servic two attempt constitut loss point',\n",
              " 'return servic receiv strike ball back hit ground second time net within boundari oppon court',\n",
              " 'like fact compani assign seat',\n",
              " 'last letter recit litani complaint momentarili stump southwest custom relat peopl bump herb kelleh ceo southwest time desk note one',\n",
              " 'phrase custom alway right origin coin harri gordon selfridg founder selfridg depart store london typic use busi convinc custom get good servic compani convinc employe give custom good servic',\n",
              " 'howev think busi abandon phrase iron lead wors custom servic',\n",
              " 'want make sure custom employe like way continent treat made clear maxim custom alway right hold sway continent',\n",
              " 'run custom reel back loyalti employe put stuff everi day buy ticket give right abus employe',\n",
              " 'run million peopl book everi month one two peopl go unreason demand jerk choic support employe work everi day make product irat jerk demand free ticket pari ran peanut whose side go',\n",
              " 'treat employe like serf valu think support custom line even smallest problem caus resent',\n",
              " 'bethun trust peopl unreason custom like attitud balanc employe custom alway right maxim squar favor custom bad idea bethun say caus resent among employe',\n",
              " 'cours plenti exampl bad employe give lousi custom servic tri solv declar custom alway right counter product',\n",
              " 'use slogan custom alway right abus custom demand anyth right definit make employe job much harder tri rein',\n",
              " 'also mean abus peopl get better treatment condit nice peopl alway seem wrong make much sens nice nice custom keep come back',\n",
              " 'busi think custom better custom quit simpli bad busi',\n",
              " 'one servic technician arriv custom site mainten task great shock treat rude custom',\n",
              " 'finish task return offic told manag experi prompt cancel custom contract',\n",
              " 'like kelleh dismiss irat ladi kept complain somehow also kept fli southwest servicegruppen fire bad custom',\n",
              " 'note even matter financi calcul question whether either compani would make lose money custom long run',\n",
              " 'rosenbluth argu put employe first put custom first put employe first happi work employe happi work give better custom servic',\n",
              " 'coronavirus cov larg famili virus known caus ill rang common cold acut respiratori tract infect',\n",
              " 'sever infect may visibl pneumonia acut respiratori syndrom even death',\n",
              " 'outbreak sar group virus great overlook howev sinc sar mer outbreak virus studi greater detail propel vaccin research',\n",
              " 'decemb mysteri case pneumonia detect citi wuhan china hubei provinc',\n",
              " 'januari causat agent identifi new coronavirus ncov diseas later name covid',\n",
              " 'virus spread extens wuhan region china gain entri countri territori',\n",
              " 'though expert suspect virus transmit anim human mix report origin virus',\n",
              " 'treatment option avail virus limit use anti hiv drug antivir remdesivir galidesivir',\n",
              " 'contain virus recommend quarantin infect follow good hygien practic',\n",
              " 'virus signific socio econom impact global econom china like experi greater setback countri pandem due ad trade war pressur',\n",
              " 'coronavirida famili virus posit sens rna possess outer viral coat',\n",
              " 'look help electron microscop appear uniqu corona around',\n",
              " 'famili virus main caus respiratori diseas human form common cold pneumonia well respiratori infect',\n",
              " 'virus infect anim well year coronavirus cov attract limit interest research',\n",
              " 'howev sar sever acut respiratori syndrom outbreak caus sar cov coronavirus look renew interest',\n",
              " 'also happen first epidem st centuri origin guangdong provinc china',\n",
              " 'almost year later mer middl east respiratori syndrom outbreak caus mer cov',\n",
              " 'sar mer zoonot origin origin bat uniqu featur virus abil mutat rapid adapt new host',\n",
              " 'zoonot origin virus allow jump host host',\n",
              " 'coronavirus known use angiotensin convert enzym ace receptor dipeptidyl peptidas iv dpp protein gain entri cell replic']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WctoGo_ou_bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35faddb-b719-4805-a9f2-f2fc05b0db43"
      },
      "source": [
        "cv = CountVectorizer()\n",
        "\n",
        "vec = cv.fit_transform(corpus)\n",
        "\n",
        "print(vec.shape)\n",
        "print(cv.get_feature_names()) #list of all words after trimming\n",
        "print(vec.toarray()) #rows: sentence indices, columns: frequency of a word\n",
        "print(vec.toarray().sum(axis=0)) #frequency of each entry\n",
        "\n",
        "freq = vec.toarray().sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 653)\n",
            "['abandon', 'abil', 'abus', 'accident', 'accord', 'ace', 'across', 'acut', 'ad', 'adapt', 'add', 'affix', 'agent', 'agre', 'akihiko', 'allow', 'almost', 'also', 'altern', 'although', 'alway', 'amateur', 'among', 'ancient', 'angiotensin', 'angl', 'anim', 'anti', 'antivir', 'anyth', 'appeal', 'appear', 'argu', 'around', 'arriv', 'artifici', 'aspect', 'asphalt', 'assign', 'associ', 'astrobiologist', 'astronaut', 'attack', 'attempt', 'attitud', 'attract', 'australia', 'avail', 'award', 'back', 'bacteria', 'bacteriato', 'bad', 'balanc', 'ball', 'band', 'baselin', 'bat', 'becam', 'began', 'begin', 'behind', 'bethun', 'better', 'bodi', 'book', 'bounc', 'boundari', 'britain', 'broadcast', 'broaden', 'bump', 'busi', 'buy', 'calcul', 'calgari', 'call', 'canada', 'cancel', 'case', 'caus', 'causat', 'cell', 'cement', 'centr', 'centuri', 'ceo', 'championship', 'chang', 'chess', 'china', 'choic', 'choos', 'circuit', 'citi', 'clay', 'clear', 'cloth', 'cm', 'coat', 'coin', 'cold', 'come', 'common', 'communiti', 'compani', 'competit', 'complain', 'complaint', 'complex', 'condit', 'consist', 'constant', 'constitut', 'contain', 'continent', 'continu', 'contract', 'convert', 'convinc', 'core', 'corona', 'coronavirida', 'coronavirus', 'correct', 'corset', 'could', 'counter', 'countri', 'cours', 'court', 'cov', 'cover', 'covid', 'cramm', 'cup', 'cushion', 'custom', 'davi', 'day', 'de', 'dead', 'death', 'decemb', 'decid', 'declar', 'defend', 'definit', 'degrad', 'degre', 'deinococcus', 'deliv', 'demand', 'depart', 'depend', 'deriv', 'desicc', 'desk', 'destroy', 'detail', 'detect', 'diagon', 'diamet', 'die', 'dimens', 'dipeptidyl', 'disappoint', 'discolor', 'diseas', 'dismiss', 'divers', 'dna', 'doubl', 'dpp', 'dri', 'drug', 'due', 'earth', 'east', 'econom', 'either', 'electron', 'employe', 'enjoy', 'entri', 'environ', 'enzym', 'epidem', 'even', 'everi', 'evid', 'evolv', 'exampl', 'expand', 'experi', 'expert', 'exploit', 'extens', 'exterior', 'extrem', 'fact', 'factor', 'fail', 'failur', 'fall', 'famili', 'fault', 'favor', 'featur', 'fed', 'feder', 'feet', 'financi', 'find', 'finish', 'fire', 'first', 'five', 'flannel', 'flew', 'fli', 'flight', 'follow', 'food', 'form', 'formal', 'founder', 'four', 'free', 'french', 'frequent', 'fri', 'friend', 'gain', 'galidesivir', 'game', 'garden', 'genet', 'gentlemen', 'get', 'give', 'global', 'go', 'good', 'gordon', 'govern', 'gradual', 'gram', 'grass', 'great', 'greater', 'ground', 'group', 'growth', 'guangdong', 'hand', 'handbal', 'happen', 'happi', 'hard', 'harder', 'harri', 'harsh', 'hazard', 'heart', 'height', 'help', 'herb', 'high', 'hit', 'hiv', 'hold', 'home', 'host', 'howev', 'hubei', 'human', 'hygien', 'idea', 'identifi', 'ill', 'impact', 'inch', 'indoor', 'infect', 'inner', 'insid', 'institut', 'interest', 'intern', 'interv', 'introduc', 'irat', 'iron', 'itf', 'iv', 'januari', 'japan', 'jerk', 'jeu', 'job', 'jump', 'keep', 'kelleh', 'kelli', 'kept', 'known', 'ladi', 'larg', 'larger', 'last', 'late', 'later', 'lawn', 'layer', 'lead', 'let', 'letter', 'level', 'life', 'like', 'limit', 'line', 'litani', 'london', 'long', 'look', 'lose', 'loss', 'lousi', 'low', 'loyalti', 'made', 'main', 'mainten', 'major', 'make', 'manag', 'margaret', 'match', 'materi', 'matter', 'maxim', 'may', 'mean', 'medal', 'membran', 'men', 'mer', 'metal', 'meteorit', 'metr', 'microb', 'microbiologist', 'micromet', 'microscop', 'middl', 'million', 'miss', 'mix', 'moisten', 'momentarili', 'money', 'month', 'much', 'must', 'mutat', 'mysteri', 'name', 'nasa', 'nation', 'ncov', 'net', 'new', 'nice', 'notabl', 'note', 'novel', 'number', 'nylon', 'odd', 'offic', 'offici', 'olymp', 'one', 'open', 'oper', 'oppon', 'oppos', 'option', 'organ', 'origin', 'ounc', 'outbreak', 'outer', 'outsid', 'overlook', 'overse', 'pace', 'pair', 'pal', 'palm', 'pandem', 'paper', 'pari', 'part', 'parti', 'particip', 'paum', 'peanut', 'pellet', 'pen', 'peopl', 'peptidas', 'percent', 'period', 'petticoat', 'phrase', 'physic', 'place', 'planet', 'plate', 'play', 'player', 'plenti', 'pneumonia', 'point', 'posit', 'possess', 'post', 'practic', 'prescrib', 'pressur', 'problem', 'product', 'profession', 'prompt', 'propel', 'protein', 'provinc', 'put', 'qualiti', 'quarantin', 'question', 'quick', 'quit', 'racket', 'radiat', 'ran', 'rang', 'rapid', 'real', 'receiv', 'receptor', 'recit', 'recommend', 'rectangular', 'reel', 'refer', 'region', 'regular', 'rein', 'relat', 'remdesivir', 'renew', 'replay', 'replic', 'report', 'research', 'resent', 'resist', 'respiratori', 'restor', 'return', 'rich', 'right', 'rise', 'rivalri', 'rna', 'rosenbluth', 'royal', 'rubber', 'rude', 'rule', 'run', 'sampl', 'sanction', 'sar', 'say', 'scienc', 'scott', 'seam', 'seat', 'second', 'seem', 'selfridg', 'sens', 'sent', 'serf', 'serv', 'server', 'servic', 'servicegruppen', 'setback', 'seven', 'sever', 'sheet', 'shield', 'shock', 'shot', 'show', 'side', 'signific', 'simpli', 'sinc', 'singl', 'site', 'size', 'skill', 'slogan', 'small', 'smallest', 'socio', 'soft', 'solv', 'somehow', 'soon', 'southwest', 'space', 'specifi', 'spectat', 'spin', 'sport', 'spread', 'squar', 'st', 'stamina', 'starch', 'state', 'station', 'status', 'stay', 'still', 'stitchless', 'store', 'strateg', 'stratospher', 'strike', 'stroke', 'strung', 'studi', 'stuf', 'stuff', 'stump', 'stylist', 'suggest', 'support', 'sure', 'surfac', 'surviv', 'suspect', 'sway', 'syndrom', 'synthet', 'take', 'task', 'taut', 'team', 'technic', 'technician', 'televis', 'temperatur', 'tenni', 'term', 'territori', 'test', 'th', 'thick', 'thin', 'think', 'though', 'three', 'thrive', 'ticket', 'time', 'today', 'togeth', 'tokyo', 'told', 'top', 'toss', 'tournament', 'trace', 'tract', 'trade', 'transmit', 'travel', 'treat', 'treatment', 'tri', 'trust', 'two', 'typic', 'ultraviolet', 'uniform', 'uniqu', 'unit', 'univers', 'unless', 'unprotect', 'unreason', 'upon', 'use', 'usual', 'vaccin', 'valu', 'varieti', 'victorian', 'viral', 'virus', 'visibl', 'void', 'wait', 'want', 'war', 'way', 'weak', 'weight', 'well', 'whalebon', 'whenev', 'whether', 'white', 'whose', 'wide', 'winner', 'within', 'withstand', 'woman', 'wool', 'work', 'world', 'worldwid', 'worri', 'wors', 'would', 'wrong', 'wrote', 'wuhan', 'yamagishi', 'year', 'yellow', 'zoonot']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[ 1  1  3  1  1  1  1  3  1  1  1  1  1  1  1  2  2  5  1  1  6  1  1  1\n",
            "  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1\n",
            "  1  5  5  1  4  1 12  1  1  1  1  1  1  2  2  3  1  1  1  1  2  1  1  1\n",
            "  4  1  1  1  4  1  1  3  6  1  3  1  1  2  1  1  2  1  5  2  2  1  1  1\n",
            "  1  1  1  1  2  2  1  3  1  4  2  1  2  1  1  1  1  2  1  2  1  1  1  2\n",
            "  1  1  1  6  3  1  2  1  2  1 16  4  1  1  2  2  1 26  1  2  1  2  1  2\n",
            "  3  1  1  1  1  1  2  1  4  1  1  2  1  1  1  1  1  1  1  1  2  1  1  1\n",
            "  2  1  1  2  2  1  1  1  1  2  1  2  2  1 13  1  2  1  1  1  3  7  1  1\n",
            "  1  1  2  1  1  1  1  3  2  1  1  1  1  3  1  1  1  1  2  6  1  2  1  1\n",
            "  8  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  2  1 13  1  1  1  2\n",
            "  4  1  3  3  1  2  1  1  2  2  2  1  3  1  1  2  1  1  2  2  1  1  1  1\n",
            "  1  1  1  1  2  2  1  1  1  3  3  2  3  1  1  1  1  1  2  1  5  1  1  1\n",
            "  2  3  1  1  2  1  3  1  1  1  2  1  1  1  1  2  1  2  5  3  1  1  1  1\n",
            "  2  3  2  1  1  1  1  4  6  3  1  1  1  3  2  1  1  1  1  1  1  1  1  1\n",
            "  7  1  1  1  2  1  2  4  1  1  1  1  5  1  1  4  6  1  2  1  1  1  1  2\n",
            "  1  1  1  1  2  3  1  1  2  1  1  2  6  4  3  1  2  1  2  1  1  1  1  1\n",
            "  5  1  1  7  1  1  1 11  1  5  5  3  1  1  1  1  1  1  1  1  1  2  1  1\n",
            "  1  1  5  1  6  1  1  1  1  2  1  4  1  2  5  7  1  3  2  1  1  1  2  1\n",
            "  3  1  2  2  1  1  1  2  4  1  1  1  1  1  3  4  1  1  2  2  3  1  1  1\n",
            "  1  1  1  2  1  1  1  1  1  1  1  1  4  2  1  6  1  3  1  9  1  1  1  1\n",
            "  1  1  1  1  3  1  1  5  5  1  1  1  1  1  1  2  2  2  1  3  2 15  1  1\n",
            "  1  2  1  2  1  1  1  6  1  1  2  2  1  1  1  1  2  1  1  1  1  2  1  4\n",
            " 10  2  1  2  3  1  1  1  1  1  2  2  1  1  3  1  1  1  1  3  1  1  3  1\n",
            "  1  1  1  2  3  1  6  5  1  1  3  1  1  2  1  2  1  1  1  1 17  1  1  1\n",
            "  2  2  1  3  1  2  1  2  3  1  1  1  1  2  1  2  1  1  1  1  1  3  2  2\n",
            "  1  3  1  1  1  2  2  1  1  1  2  2  7  2  1  1  2  1  1 14  1  1  1  1\n",
            "  1  1  1  3  4  1  1  1  2  1  1  1  2  1  1  1  4  1  1  1  1  1  1  1\n",
            "  3  2  6  1  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrCussgou3lV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe12e739-e99d-42f8-838f-891845bda933"
      },
      "source": [
        "#vocabulary\n",
        "#print(cv.vocabulary_)\n",
        "voc = cv.vocabulary_\n",
        "voc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abandon': 0,\n",
              " 'abil': 1,\n",
              " 'abus': 2,\n",
              " 'accident': 3,\n",
              " 'accord': 4,\n",
              " 'ace': 5,\n",
              " 'across': 6,\n",
              " 'acut': 7,\n",
              " 'ad': 8,\n",
              " 'adapt': 9,\n",
              " 'add': 10,\n",
              " 'affix': 11,\n",
              " 'agent': 12,\n",
              " 'agre': 13,\n",
              " 'akihiko': 14,\n",
              " 'allow': 15,\n",
              " 'almost': 16,\n",
              " 'also': 17,\n",
              " 'altern': 18,\n",
              " 'although': 19,\n",
              " 'alway': 20,\n",
              " 'amateur': 21,\n",
              " 'among': 22,\n",
              " 'ancient': 23,\n",
              " 'angiotensin': 24,\n",
              " 'angl': 25,\n",
              " 'anim': 26,\n",
              " 'anti': 27,\n",
              " 'antivir': 28,\n",
              " 'anyth': 29,\n",
              " 'appeal': 30,\n",
              " 'appear': 31,\n",
              " 'argu': 32,\n",
              " 'around': 33,\n",
              " 'arriv': 34,\n",
              " 'artifici': 35,\n",
              " 'aspect': 36,\n",
              " 'asphalt': 37,\n",
              " 'assign': 38,\n",
              " 'associ': 39,\n",
              " 'astrobiologist': 40,\n",
              " 'astronaut': 41,\n",
              " 'attack': 42,\n",
              " 'attempt': 43,\n",
              " 'attitud': 44,\n",
              " 'attract': 45,\n",
              " 'australia': 46,\n",
              " 'avail': 47,\n",
              " 'award': 48,\n",
              " 'back': 49,\n",
              " 'bacteria': 50,\n",
              " 'bacteriato': 51,\n",
              " 'bad': 52,\n",
              " 'balanc': 53,\n",
              " 'ball': 54,\n",
              " 'band': 55,\n",
              " 'baselin': 56,\n",
              " 'bat': 57,\n",
              " 'becam': 58,\n",
              " 'began': 59,\n",
              " 'begin': 60,\n",
              " 'behind': 61,\n",
              " 'bethun': 62,\n",
              " 'better': 63,\n",
              " 'bodi': 64,\n",
              " 'book': 65,\n",
              " 'bounc': 66,\n",
              " 'boundari': 67,\n",
              " 'britain': 68,\n",
              " 'broadcast': 69,\n",
              " 'broaden': 70,\n",
              " 'bump': 71,\n",
              " 'busi': 72,\n",
              " 'buy': 73,\n",
              " 'calcul': 74,\n",
              " 'calgari': 75,\n",
              " 'call': 76,\n",
              " 'canada': 77,\n",
              " 'cancel': 78,\n",
              " 'case': 79,\n",
              " 'caus': 80,\n",
              " 'causat': 81,\n",
              " 'cell': 82,\n",
              " 'cement': 83,\n",
              " 'centr': 84,\n",
              " 'centuri': 85,\n",
              " 'ceo': 86,\n",
              " 'championship': 87,\n",
              " 'chang': 88,\n",
              " 'chess': 89,\n",
              " 'china': 90,\n",
              " 'choic': 91,\n",
              " 'choos': 92,\n",
              " 'circuit': 93,\n",
              " 'citi': 94,\n",
              " 'clay': 95,\n",
              " 'clear': 96,\n",
              " 'cloth': 97,\n",
              " 'cm': 98,\n",
              " 'coat': 99,\n",
              " 'coin': 100,\n",
              " 'cold': 101,\n",
              " 'come': 102,\n",
              " 'common': 103,\n",
              " 'communiti': 104,\n",
              " 'compani': 105,\n",
              " 'competit': 106,\n",
              " 'complain': 107,\n",
              " 'complaint': 108,\n",
              " 'complex': 109,\n",
              " 'condit': 110,\n",
              " 'consist': 111,\n",
              " 'constant': 112,\n",
              " 'constitut': 113,\n",
              " 'contain': 114,\n",
              " 'continent': 115,\n",
              " 'continu': 116,\n",
              " 'contract': 117,\n",
              " 'convert': 118,\n",
              " 'convinc': 119,\n",
              " 'core': 120,\n",
              " 'corona': 121,\n",
              " 'coronavirida': 122,\n",
              " 'coronavirus': 123,\n",
              " 'correct': 124,\n",
              " 'corset': 125,\n",
              " 'could': 126,\n",
              " 'counter': 127,\n",
              " 'countri': 128,\n",
              " 'cours': 129,\n",
              " 'court': 130,\n",
              " 'cov': 131,\n",
              " 'cover': 132,\n",
              " 'covid': 133,\n",
              " 'cramm': 134,\n",
              " 'cup': 135,\n",
              " 'cushion': 136,\n",
              " 'custom': 137,\n",
              " 'davi': 138,\n",
              " 'day': 139,\n",
              " 'de': 140,\n",
              " 'dead': 141,\n",
              " 'death': 142,\n",
              " 'decemb': 143,\n",
              " 'decid': 144,\n",
              " 'declar': 145,\n",
              " 'defend': 146,\n",
              " 'definit': 147,\n",
              " 'degrad': 148,\n",
              " 'degre': 149,\n",
              " 'deinococcus': 150,\n",
              " 'deliv': 151,\n",
              " 'demand': 152,\n",
              " 'depart': 153,\n",
              " 'depend': 154,\n",
              " 'deriv': 155,\n",
              " 'desicc': 156,\n",
              " 'desk': 157,\n",
              " 'destroy': 158,\n",
              " 'detail': 159,\n",
              " 'detect': 160,\n",
              " 'diagon': 161,\n",
              " 'diamet': 162,\n",
              " 'die': 163,\n",
              " 'dimens': 164,\n",
              " 'dipeptidyl': 165,\n",
              " 'disappoint': 166,\n",
              " 'discolor': 167,\n",
              " 'diseas': 168,\n",
              " 'dismiss': 169,\n",
              " 'divers': 170,\n",
              " 'dna': 171,\n",
              " 'doubl': 172,\n",
              " 'dpp': 173,\n",
              " 'dri': 174,\n",
              " 'drug': 175,\n",
              " 'due': 176,\n",
              " 'earth': 177,\n",
              " 'east': 178,\n",
              " 'econom': 179,\n",
              " 'either': 180,\n",
              " 'electron': 181,\n",
              " 'employe': 182,\n",
              " 'enjoy': 183,\n",
              " 'entri': 184,\n",
              " 'environ': 185,\n",
              " 'enzym': 186,\n",
              " 'epidem': 187,\n",
              " 'even': 188,\n",
              " 'everi': 189,\n",
              " 'evid': 190,\n",
              " 'evolv': 191,\n",
              " 'exampl': 192,\n",
              " 'expand': 193,\n",
              " 'experi': 194,\n",
              " 'expert': 195,\n",
              " 'exploit': 196,\n",
              " 'extens': 197,\n",
              " 'exterior': 198,\n",
              " 'extrem': 199,\n",
              " 'fact': 200,\n",
              " 'factor': 201,\n",
              " 'fail': 202,\n",
              " 'failur': 203,\n",
              " 'fall': 204,\n",
              " 'famili': 205,\n",
              " 'fault': 206,\n",
              " 'favor': 207,\n",
              " 'featur': 208,\n",
              " 'fed': 209,\n",
              " 'feder': 210,\n",
              " 'feet': 211,\n",
              " 'financi': 212,\n",
              " 'find': 213,\n",
              " 'finish': 214,\n",
              " 'fire': 215,\n",
              " 'first': 216,\n",
              " 'five': 217,\n",
              " 'flannel': 218,\n",
              " 'flew': 219,\n",
              " 'fli': 220,\n",
              " 'flight': 221,\n",
              " 'follow': 222,\n",
              " 'food': 223,\n",
              " 'form': 224,\n",
              " 'formal': 225,\n",
              " 'founder': 226,\n",
              " 'four': 227,\n",
              " 'free': 228,\n",
              " 'french': 229,\n",
              " 'frequent': 230,\n",
              " 'fri': 231,\n",
              " 'friend': 232,\n",
              " 'gain': 233,\n",
              " 'galidesivir': 234,\n",
              " 'game': 235,\n",
              " 'garden': 236,\n",
              " 'genet': 237,\n",
              " 'gentlemen': 238,\n",
              " 'get': 239,\n",
              " 'give': 240,\n",
              " 'global': 241,\n",
              " 'go': 242,\n",
              " 'good': 243,\n",
              " 'gordon': 244,\n",
              " 'govern': 245,\n",
              " 'gradual': 246,\n",
              " 'gram': 247,\n",
              " 'grass': 248,\n",
              " 'great': 249,\n",
              " 'greater': 250,\n",
              " 'ground': 251,\n",
              " 'group': 252,\n",
              " 'growth': 253,\n",
              " 'guangdong': 254,\n",
              " 'hand': 255,\n",
              " 'handbal': 256,\n",
              " 'happen': 257,\n",
              " 'happi': 258,\n",
              " 'hard': 259,\n",
              " 'harder': 260,\n",
              " 'harri': 261,\n",
              " 'harsh': 262,\n",
              " 'hazard': 263,\n",
              " 'heart': 264,\n",
              " 'height': 265,\n",
              " 'help': 266,\n",
              " 'herb': 267,\n",
              " 'high': 268,\n",
              " 'hit': 269,\n",
              " 'hiv': 270,\n",
              " 'hold': 271,\n",
              " 'home': 272,\n",
              " 'host': 273,\n",
              " 'howev': 274,\n",
              " 'hubei': 275,\n",
              " 'human': 276,\n",
              " 'hygien': 277,\n",
              " 'idea': 278,\n",
              " 'identifi': 279,\n",
              " 'ill': 280,\n",
              " 'impact': 281,\n",
              " 'inch': 282,\n",
              " 'indoor': 283,\n",
              " 'infect': 284,\n",
              " 'inner': 285,\n",
              " 'insid': 286,\n",
              " 'institut': 287,\n",
              " 'interest': 288,\n",
              " 'intern': 289,\n",
              " 'interv': 290,\n",
              " 'introduc': 291,\n",
              " 'irat': 292,\n",
              " 'iron': 293,\n",
              " 'itf': 294,\n",
              " 'iv': 295,\n",
              " 'januari': 296,\n",
              " 'japan': 297,\n",
              " 'jerk': 298,\n",
              " 'jeu': 299,\n",
              " 'job': 300,\n",
              " 'jump': 301,\n",
              " 'keep': 302,\n",
              " 'kelleh': 303,\n",
              " 'kelli': 304,\n",
              " 'kept': 305,\n",
              " 'known': 306,\n",
              " 'ladi': 307,\n",
              " 'larg': 308,\n",
              " 'larger': 309,\n",
              " 'last': 310,\n",
              " 'late': 311,\n",
              " 'later': 312,\n",
              " 'lawn': 313,\n",
              " 'layer': 314,\n",
              " 'lead': 315,\n",
              " 'let': 316,\n",
              " 'letter': 317,\n",
              " 'level': 318,\n",
              " 'life': 319,\n",
              " 'like': 320,\n",
              " 'limit': 321,\n",
              " 'line': 322,\n",
              " 'litani': 323,\n",
              " 'london': 324,\n",
              " 'long': 325,\n",
              " 'look': 326,\n",
              " 'lose': 327,\n",
              " 'loss': 328,\n",
              " 'lousi': 329,\n",
              " 'low': 330,\n",
              " 'loyalti': 331,\n",
              " 'made': 332,\n",
              " 'main': 333,\n",
              " 'mainten': 334,\n",
              " 'major': 335,\n",
              " 'make': 336,\n",
              " 'manag': 337,\n",
              " 'margaret': 338,\n",
              " 'match': 339,\n",
              " 'materi': 340,\n",
              " 'matter': 341,\n",
              " 'maxim': 342,\n",
              " 'may': 343,\n",
              " 'mean': 344,\n",
              " 'medal': 345,\n",
              " 'membran': 346,\n",
              " 'men': 347,\n",
              " 'mer': 348,\n",
              " 'metal': 349,\n",
              " 'meteorit': 350,\n",
              " 'metr': 351,\n",
              " 'microb': 352,\n",
              " 'microbiologist': 353,\n",
              " 'micromet': 354,\n",
              " 'microscop': 355,\n",
              " 'middl': 356,\n",
              " 'million': 357,\n",
              " 'miss': 358,\n",
              " 'mix': 359,\n",
              " 'moisten': 360,\n",
              " 'momentarili': 361,\n",
              " 'money': 362,\n",
              " 'month': 363,\n",
              " 'much': 364,\n",
              " 'must': 365,\n",
              " 'mutat': 366,\n",
              " 'mysteri': 367,\n",
              " 'name': 368,\n",
              " 'nasa': 369,\n",
              " 'nation': 370,\n",
              " 'ncov': 371,\n",
              " 'net': 372,\n",
              " 'new': 373,\n",
              " 'nice': 374,\n",
              " 'notabl': 375,\n",
              " 'note': 376,\n",
              " 'novel': 377,\n",
              " 'number': 378,\n",
              " 'nylon': 379,\n",
              " 'odd': 380,\n",
              " 'offic': 381,\n",
              " 'offici': 382,\n",
              " 'olymp': 383,\n",
              " 'one': 384,\n",
              " 'open': 385,\n",
              " 'oper': 386,\n",
              " 'oppon': 387,\n",
              " 'oppos': 388,\n",
              " 'option': 389,\n",
              " 'organ': 390,\n",
              " 'origin': 391,\n",
              " 'ounc': 392,\n",
              " 'outbreak': 393,\n",
              " 'outer': 394,\n",
              " 'outsid': 395,\n",
              " 'overlook': 396,\n",
              " 'overse': 397,\n",
              " 'pace': 398,\n",
              " 'pair': 399,\n",
              " 'pal': 400,\n",
              " 'palm': 401,\n",
              " 'pandem': 402,\n",
              " 'paper': 403,\n",
              " 'pari': 404,\n",
              " 'part': 405,\n",
              " 'parti': 406,\n",
              " 'particip': 407,\n",
              " 'paum': 408,\n",
              " 'peanut': 409,\n",
              " 'pellet': 410,\n",
              " 'pen': 411,\n",
              " 'peopl': 412,\n",
              " 'peptidas': 413,\n",
              " 'percent': 414,\n",
              " 'period': 415,\n",
              " 'petticoat': 416,\n",
              " 'phrase': 417,\n",
              " 'physic': 418,\n",
              " 'place': 419,\n",
              " 'planet': 420,\n",
              " 'plate': 421,\n",
              " 'play': 422,\n",
              " 'player': 423,\n",
              " 'plenti': 424,\n",
              " 'pneumonia': 425,\n",
              " 'point': 426,\n",
              " 'posit': 427,\n",
              " 'possess': 428,\n",
              " 'post': 429,\n",
              " 'practic': 430,\n",
              " 'prescrib': 431,\n",
              " 'pressur': 432,\n",
              " 'problem': 433,\n",
              " 'product': 434,\n",
              " 'profession': 435,\n",
              " 'prompt': 436,\n",
              " 'propel': 437,\n",
              " 'protein': 438,\n",
              " 'provinc': 439,\n",
              " 'put': 440,\n",
              " 'qualiti': 441,\n",
              " 'quarantin': 442,\n",
              " 'question': 443,\n",
              " 'quick': 444,\n",
              " 'quit': 445,\n",
              " 'racket': 446,\n",
              " 'radiat': 447,\n",
              " 'ran': 448,\n",
              " 'rang': 449,\n",
              " 'rapid': 450,\n",
              " 'real': 451,\n",
              " 'receiv': 452,\n",
              " 'receptor': 453,\n",
              " 'recit': 454,\n",
              " 'recommend': 455,\n",
              " 'rectangular': 456,\n",
              " 'reel': 457,\n",
              " 'refer': 458,\n",
              " 'region': 459,\n",
              " 'regular': 460,\n",
              " 'rein': 461,\n",
              " 'relat': 462,\n",
              " 'remdesivir': 463,\n",
              " 'renew': 464,\n",
              " 'replay': 465,\n",
              " 'replic': 466,\n",
              " 'report': 467,\n",
              " 'research': 468,\n",
              " 'resent': 469,\n",
              " 'resist': 470,\n",
              " 'respiratori': 471,\n",
              " 'restor': 472,\n",
              " 'return': 473,\n",
              " 'rich': 474,\n",
              " 'right': 475,\n",
              " 'rise': 476,\n",
              " 'rivalri': 477,\n",
              " 'rna': 478,\n",
              " 'rosenbluth': 479,\n",
              " 'royal': 480,\n",
              " 'rubber': 481,\n",
              " 'rude': 482,\n",
              " 'rule': 483,\n",
              " 'run': 484,\n",
              " 'sampl': 485,\n",
              " 'sanction': 486,\n",
              " 'sar': 487,\n",
              " 'say': 488,\n",
              " 'scienc': 489,\n",
              " 'scott': 490,\n",
              " 'seam': 491,\n",
              " 'seat': 492,\n",
              " 'second': 493,\n",
              " 'seem': 494,\n",
              " 'selfridg': 495,\n",
              " 'sens': 496,\n",
              " 'sent': 497,\n",
              " 'serf': 498,\n",
              " 'serv': 499,\n",
              " 'server': 500,\n",
              " 'servic': 501,\n",
              " 'servicegruppen': 502,\n",
              " 'setback': 503,\n",
              " 'seven': 504,\n",
              " 'sever': 505,\n",
              " 'sheet': 506,\n",
              " 'shield': 507,\n",
              " 'shock': 508,\n",
              " 'shot': 509,\n",
              " 'show': 510,\n",
              " 'side': 511,\n",
              " 'signific': 512,\n",
              " 'simpli': 513,\n",
              " 'sinc': 514,\n",
              " 'singl': 515,\n",
              " 'site': 516,\n",
              " 'size': 517,\n",
              " 'skill': 518,\n",
              " 'slogan': 519,\n",
              " 'small': 520,\n",
              " 'smallest': 521,\n",
              " 'socio': 522,\n",
              " 'soft': 523,\n",
              " 'solv': 524,\n",
              " 'somehow': 525,\n",
              " 'soon': 526,\n",
              " 'southwest': 527,\n",
              " 'space': 528,\n",
              " 'specifi': 529,\n",
              " 'spectat': 530,\n",
              " 'spin': 531,\n",
              " 'sport': 532,\n",
              " 'spread': 533,\n",
              " 'squar': 534,\n",
              " 'st': 535,\n",
              " 'stamina': 536,\n",
              " 'starch': 537,\n",
              " 'state': 538,\n",
              " 'station': 539,\n",
              " 'status': 540,\n",
              " 'stay': 541,\n",
              " 'still': 542,\n",
              " 'stitchless': 543,\n",
              " 'store': 544,\n",
              " 'strateg': 545,\n",
              " 'stratospher': 546,\n",
              " 'strike': 547,\n",
              " 'stroke': 548,\n",
              " 'strung': 549,\n",
              " 'studi': 550,\n",
              " 'stuf': 551,\n",
              " 'stuff': 552,\n",
              " 'stump': 553,\n",
              " 'stylist': 554,\n",
              " 'suggest': 555,\n",
              " 'support': 556,\n",
              " 'sure': 557,\n",
              " 'surfac': 558,\n",
              " 'surviv': 559,\n",
              " 'suspect': 560,\n",
              " 'sway': 561,\n",
              " 'syndrom': 562,\n",
              " 'synthet': 563,\n",
              " 'take': 564,\n",
              " 'task': 565,\n",
              " 'taut': 566,\n",
              " 'team': 567,\n",
              " 'technic': 568,\n",
              " 'technician': 569,\n",
              " 'televis': 570,\n",
              " 'temperatur': 571,\n",
              " 'tenni': 572,\n",
              " 'term': 573,\n",
              " 'territori': 574,\n",
              " 'test': 575,\n",
              " 'th': 576,\n",
              " 'thick': 577,\n",
              " 'thin': 578,\n",
              " 'think': 579,\n",
              " 'though': 580,\n",
              " 'three': 581,\n",
              " 'thrive': 582,\n",
              " 'ticket': 583,\n",
              " 'time': 584,\n",
              " 'today': 585,\n",
              " 'togeth': 586,\n",
              " 'tokyo': 587,\n",
              " 'told': 588,\n",
              " 'top': 589,\n",
              " 'toss': 590,\n",
              " 'tournament': 591,\n",
              " 'trace': 592,\n",
              " 'tract': 593,\n",
              " 'trade': 594,\n",
              " 'transmit': 595,\n",
              " 'travel': 596,\n",
              " 'treat': 597,\n",
              " 'treatment': 598,\n",
              " 'tri': 599,\n",
              " 'trust': 600,\n",
              " 'two': 601,\n",
              " 'typic': 602,\n",
              " 'ultraviolet': 603,\n",
              " 'uniform': 604,\n",
              " 'uniqu': 605,\n",
              " 'unit': 606,\n",
              " 'univers': 607,\n",
              " 'unless': 608,\n",
              " 'unprotect': 609,\n",
              " 'unreason': 610,\n",
              " 'upon': 611,\n",
              " 'use': 612,\n",
              " 'usual': 613,\n",
              " 'vaccin': 614,\n",
              " 'valu': 615,\n",
              " 'varieti': 616,\n",
              " 'victorian': 617,\n",
              " 'viral': 618,\n",
              " 'virus': 619,\n",
              " 'visibl': 620,\n",
              " 'void': 621,\n",
              " 'wait': 622,\n",
              " 'want': 623,\n",
              " 'war': 624,\n",
              " 'way': 625,\n",
              " 'weak': 626,\n",
              " 'weight': 627,\n",
              " 'well': 628,\n",
              " 'whalebon': 629,\n",
              " 'whenev': 630,\n",
              " 'whether': 631,\n",
              " 'white': 632,\n",
              " 'whose': 633,\n",
              " 'wide': 634,\n",
              " 'winner': 635,\n",
              " 'within': 636,\n",
              " 'withstand': 637,\n",
              " 'woman': 638,\n",
              " 'wool': 639,\n",
              " 'work': 640,\n",
              " 'world': 641,\n",
              " 'worldwid': 642,\n",
              " 'worri': 643,\n",
              " 'wors': 644,\n",
              " 'would': 645,\n",
              " 'wrong': 646,\n",
              " 'wrote': 647,\n",
              " 'wuhan': 648,\n",
              " 'yamagishi': 649,\n",
              " 'year': 650,\n",
              " 'yellow': 651,\n",
              " 'zoonot': 652}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb6vkZUaurRi"
      },
      "source": [
        "1.2 Prior Distribution of Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep4BFCsnzQr4"
      },
      "source": [
        "lab_val = {'science':0, 'sports':1, 'business':2, 'covid':3}\n",
        "numlabel = 4 #number of possible labels\n",
        "lab_freq = np.zeros(numlabel)\n",
        "all_lab = dataset['category'] # all labels as they appear in data\n",
        "  # number of feature vectors available for training\n",
        "\n",
        "for i in range(0, numfv):\n",
        "  for j in lab_val:\n",
        "    if all_lab[i] == j:\n",
        "      lab_freq[lab_val[j]] = lab_freq[lab_val[j]]+1\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsmVtULBvXqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595cfdf9-b5d4-4d59-a335-461e30605607"
      },
      "source": [
        "s = sum(lab_freq)\n",
        "prior_dis = (1/s)*lab_freq\n",
        "prior_dis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25  , 0.25  , 0.2375, 0.2625])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7TrOp3X2YSs"
      },
      "source": [
        "Part 1.3 Class-Conditional Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9RiQaPNFAuW"
      },
      "source": [
        "D = np.shape(vec)[1] # number of all possible words\n",
        "\n",
        "dist = np.zeros((numlabel, D)) # to store occurence of each word in each class\n",
        "p_xgy = np.zeros((numlabel, D)) # p_xgy := probability of x given y so it is for class-conditional distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7HGGDixJOST"
      },
      "source": [
        "for i in lab_val: # i is one of 4 labels \n",
        "  temp = np.zeros(D)         # to store all entries of one label\n",
        "  for j in range(0, numfv):      \n",
        "    if dataset['category'][j] == i: # find all labels same as i-th \n",
        "      temp = temp + vec.toarray()[j]          # add all of them 1-by-1\n",
        "  tot = sum(temp)\n",
        "  dist[lab_val[i]] = temp\n",
        "  temp = temp/tot #to calculate the probability\n",
        "  p_xgy[lab_val[i]] = temp #class-conditional prob.\n",
        "\n",
        "#patchwork\n",
        "for i in range(0, numlabel): #for i-th label\n",
        "    for j in range(0, len(p_xgy[i])): #check each word\n",
        "        temp = -1\n",
        "        if p_xgy[i,j] > 0: \n",
        "            temp = j #first word with non-zero prob., save it\n",
        "        else:\n",
        "            p_xgy[i, j] = 0.00000001 #adjust zero or -ve prob.\n",
        "            p_xgy[i, temp] = p_xgy[i, temp] - 0.00000001 #adjusting previous non-zero entry for proper normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOhLDHdznatg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6042de5-5ea1-4a8f-9147-cd6c48183b31"
      },
      "source": [
        "#p_xgy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e-08, 1.00000000e-08, 1.00000000e-08, ...,\n",
              "        1.49253731e-02, 1.00000000e-08, 0.00000000e+00],\n",
              "       [1.00000000e-08, 1.00000000e-08, 1.00000000e-08, ...,\n",
              "        1.00000000e-08, 2.33644860e-03, 0.00000000e+00],\n",
              "       [3.03030303e-03, 1.00000000e-08, 9.09090909e-03, ...,\n",
              "        1.00000000e-08, 1.00000000e-08, 0.00000000e+00],\n",
              "       [1.00000000e-08, 3.89105058e-03, 1.00000000e-08, ...,\n",
              "        1.16731518e-02, 1.00000000e-08, 7.77718117e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYKNX8akPZhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031e8510-50cc-419a-dbce-50711fedfc6f"
      },
      "source": [
        "#sample: how to get a class-conditional probability\n",
        "#to class-conditional prob. for word 'virus' in class covid\n",
        "\n",
        "p_xgy[lab_val['covid'], voc['virus']] # p_xgy[y, x]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.054474708171206226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh-avrkxVgBb"
      },
      "source": [
        "1.4 Posterior Distribution over Test Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3llyfGLRwTj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "bf8e16b5-630f-424c-8cce-741e48100c3e"
      },
      "source": [
        "#read test data\n",
        "dataset2 = pd.read_csv('testdata.csv')\n",
        "dataset2 = dataset2.iloc[: , 0: 2]\n",
        "numfv2 = np.shape(dataset2)[0] #num. of feature vectors for testing\n",
        "\n",
        "dataset2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>science</td>\n",
              "      <td>He estimates that 1,000-micrometer pellets cou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>science</td>\n",
              "      <td>Thats enough time to potentially get to Mars...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>science</td>\n",
              "      <td>How exactly clumps of microbes might get expel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>science</td>\n",
              "      <td>The microbes might get kicked up by small mete...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>science</td>\n",
              "      <td>Someday, if microbial life is ever discovered ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>sports</td>\n",
              "      <td>In tennis, After the service has been correctl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sports</td>\n",
              "      <td>This may occur if a tennis player fails to hit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>sports</td>\n",
              "      <td>To win a game, a tennis player must win four p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>sports</td>\n",
              "      <td>In tennis, It never has been satisfactorily ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>sports</td>\n",
              "      <td>In tennis, The servers score is called first;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>business</td>\n",
              "      <td>Herb Kelleher makes it clear that his employee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>business</td>\n",
              "      <td>We dont carry those sorts of customers. We wr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>business</td>\n",
              "      <td>A Continental flight attendant once was offend...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>business</td>\n",
              "      <td>It was pretty offensive stuff, so the attendan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>business</td>\n",
              "      <td>The fact is that some customers are just plain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>covid</td>\n",
              "      <td>the virus appeared to be contained within Chin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>covid</td>\n",
              "      <td>However, as of April 2020, over 210 countries ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>covid</td>\n",
              "      <td>Similarly, SARS and MERS were also suspected t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>covid</td>\n",
              "      <td>Bats have been known to harbor coronaviruses f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>covid</td>\n",
              "      <td>The identification of the original host helps ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    category                                               text\n",
              "0    science  He estimates that 1,000-micrometer pellets cou...\n",
              "1    science  Thats enough time to potentially get to Mars...\n",
              "2    science  How exactly clumps of microbes might get expel...\n",
              "3    science  The microbes might get kicked up by small mete...\n",
              "4    science  Someday, if microbial life is ever discovered ...\n",
              "5     sports  In tennis, After the service has been correctl...\n",
              "6     sports  This may occur if a tennis player fails to hit...\n",
              "7     sports  To win a game, a tennis player must win four p...\n",
              "8     sports  In tennis, It never has been satisfactorily ex...\n",
              "9     sports  In tennis, The servers score is called first;...\n",
              "10  business  Herb Kelleher makes it clear that his employee...\n",
              "11  business  We dont carry those sorts of customers. We wr...\n",
              "12  business  A Continental flight attendant once was offend...\n",
              "13  business  It was pretty offensive stuff, so the attendan...\n",
              "14  business  The fact is that some customers are just plain...\n",
              "15     covid  the virus appeared to be contained within Chin...\n",
              "16     covid  However, as of April 2020, over 210 countries ...\n",
              "17     covid  Similarly, SARS and MERS were also suspected t...\n",
              "18     covid  Bats have been known to harbor coronaviruses f...\n",
              "19     covid  The identification of the original host helps ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8UsR1i7WZgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16f89e9-35bf-41ec-d2fc-bc2652870a12"
      },
      "source": [
        "corpus2 = [] #to contain the stemmed words without punctuations\n",
        "\n",
        "for i in range(0, numfv2): \n",
        "  news = re.sub('[^a-zA-Z]', ' ', dataset2['text'][i]) #only alphabets are retained\n",
        "  news = news.lower() \n",
        "  news = news.split()\n",
        "  news = [sno.stem(word) for word in news if not word in set(all_stopwords)] \n",
        "  #news = ' '.join(news) #DO NOT JOIN, separate words needed for probability calculation\n",
        "  corpus2.append(news)\n",
        "    \n",
        "corpus2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['estim',\n",
              "  'micromet',\n",
              "  'pellet',\n",
              "  'could',\n",
              "  'surviv',\n",
              "  'eight',\n",
              "  'year',\n",
              "  'float',\n",
              "  'space'],\n",
              " ['enough',\n",
              "  'time',\n",
              "  'potenti',\n",
              "  'get',\n",
              "  'mar',\n",
              "  'say',\n",
              "  'rare',\n",
              "  'meteor',\n",
              "  'might',\n",
              "  'even',\n",
              "  'abl',\n",
              "  'travel',\n",
              "  'mar',\n",
              "  'earth',\n",
              "  'month',\n",
              "  'year'],\n",
              " ['exact',\n",
              "  'clump',\n",
              "  'microb',\n",
              "  'might',\n",
              "  'get',\n",
              "  'expel',\n",
              "  'space',\n",
              "  'clear',\n",
              "  'trip',\n",
              "  'could',\n",
              "  'happen',\n",
              "  'say'],\n",
              " ['microb',\n",
              "  'might',\n",
              "  'get',\n",
              "  'kick',\n",
              "  'small',\n",
              "  'meteorit',\n",
              "  'might',\n",
              "  'thrown',\n",
              "  'earth',\n",
              "  'space',\n",
              "  'thunderstorm',\n",
              "  'induc',\n",
              "  'perturb',\n",
              "  'earth',\n",
              "  'magnet',\n",
              "  'field',\n",
              "  'yamagishi',\n",
              "  'say'],\n",
              " ['someday',\n",
              "  'microbi',\n",
              "  'life',\n",
              "  'ever',\n",
              "  'discov',\n",
              "  'mar',\n",
              "  'hope',\n",
              "  'look',\n",
              "  'evid',\n",
              "  'journey',\n",
              "  'ultim',\n",
              "  'dream'],\n",
              " ['tenni',\n",
              "  'servic',\n",
              "  'correct',\n",
              "  'return',\n",
              "  'player',\n",
              "  'may',\n",
              "  'volley',\n",
              "  'ball',\n",
              "  'e',\n",
              "  'hit',\n",
              "  'bounc',\n",
              "  'hit',\n",
              "  'first',\n",
              "  'bounc',\n",
              "  'point',\n",
              "  'continu',\n",
              "  'one',\n",
              "  'player',\n",
              "  'fail',\n",
              "  'make',\n",
              "  'correct',\n",
              "  'return'],\n",
              " ['may',\n",
              "  'occur',\n",
              "  'tenni',\n",
              "  'player',\n",
              "  'fail',\n",
              "  'hit',\n",
              "  'ball',\n",
              "  'net',\n",
              "  'hit',\n",
              "  'outsid',\n",
              "  'oppon',\n",
              "  'boundari',\n",
              "  'fail',\n",
              "  'hit',\n",
              "  'strike',\n",
              "  'ground',\n",
              "  'second',\n",
              "  'time',\n",
              "  'side',\n",
              "  'net'],\n",
              " ['win',\n",
              "  'game',\n",
              "  'tenni',\n",
              "  'player',\n",
              "  'must',\n",
              "  'win',\n",
              "  'four',\n",
              "  'point',\n",
              "  'margin',\n",
              "  'two',\n",
              "  'score',\n",
              "  'goe',\n",
              "  'game',\n",
              "  'system',\n",
              "  'deriv',\n",
              "  'real',\n",
              "  'tenni',\n",
              "  'mediev',\n",
              "  'origin'],\n",
              " ['tenni',\n",
              "  'never',\n",
              "  'satisfactorili',\n",
              "  'explain',\n",
              "  'three',\n",
              "  'point',\n",
              "  'equal',\n",
              "  'rather',\n",
              "  'zero',\n",
              "  'general',\n",
              "  'refer',\n",
              "  'love',\n",
              "  'thought',\n",
              "  'deriv',\n",
              "  'l',\n",
              "  'oeuf',\n",
              "  'french',\n",
              "  'word',\n",
              "  'egg'],\n",
              " ['tenni',\n",
              "  'server',\n",
              "  'score',\n",
              "  'call',\n",
              "  'first',\n",
              "  'thus',\n",
              "  'mean',\n",
              "  'server',\n",
              "  'two',\n",
              "  'point',\n",
              "  'one',\n",
              "  'wherea',\n",
              "  'mean',\n",
              "  'receiv',\n",
              "  'two',\n",
              "  'point',\n",
              "  'one'],\n",
              " ['herb',\n",
              "  'kelleh',\n",
              "  'make',\n",
              "  'clear',\n",
              "  'employe',\n",
              "  'come',\n",
              "  'first',\n",
              "  'even',\n",
              "  'mean',\n",
              "  'dismiss',\n",
              "  'custom',\n",
              "  'custom',\n",
              "  'alway',\n",
              "  'right',\n",
              "  'kelleh',\n",
              "  'snap',\n",
              "  'think',\n",
              "  'one',\n",
              "  'biggest',\n",
              "  'betray',\n",
              "  'employe',\n",
              "  'boss',\n",
              "  'possibl',\n",
              "  'commit',\n",
              "  'custom',\n",
              "  'sometim',\n",
              "  'wrong'],\n",
              " ['carri',\n",
              "  'sort',\n",
              "  'custom',\n",
              "  'write',\n",
              "  'say',\n",
              "  'fli',\n",
              "  'somebodi',\n",
              "  'els',\n",
              "  'abus',\n",
              "  'peopl'],\n",
              " ['continent',\n",
              "  'flight',\n",
              "  'attend',\n",
              "  'offend',\n",
              "  'passeng',\n",
              "  'child',\n",
              "  'wear',\n",
              "  'hat',\n",
              "  'nazi',\n",
              "  'kkk',\n",
              "  'emblem'],\n",
              " ['pretti',\n",
              "  'offens',\n",
              "  'stuff',\n",
              "  'attend',\n",
              "  'went',\n",
              "  'kid',\n",
              "  'father',\n",
              "  'ask',\n",
              "  'put',\n",
              "  'away',\n",
              "  'hat',\n",
              "  'guy',\n",
              "  'said',\n",
              "  'kid',\n",
              "  'wear',\n",
              "  'want',\n",
              "  'care',\n",
              "  'like'],\n",
              " ['fact',\n",
              "  'custom',\n",
              "  'plain',\n",
              "  'wrong',\n",
              "  'busi',\n",
              "  'better',\n",
              "  'without',\n",
              "  'manag',\n",
              "  'side',\n",
              "  'unreason',\n",
              "  'custom',\n",
              "  'employe',\n",
              "  'bad',\n",
              "  'idea',\n",
              "  'result',\n",
              "  'wors',\n",
              "  'custom',\n",
              "  'servic'],\n",
              " ['virus',\n",
              "  'appear',\n",
              "  'contain',\n",
              "  'within',\n",
              "  'china',\n",
              "  'cruis',\n",
              "  'ship',\n",
              "  'diamond',\n",
              "  'princess',\n",
              "  'form',\n",
              "  'major',\n",
              "  'cluster',\n",
              "  'virus'],\n",
              " ['howev',\n",
              "  'april',\n",
              "  'countri',\n",
              "  'territori',\n",
              "  'affect',\n",
              "  'virus',\n",
              "  'europ',\n",
              "  'usa',\n",
              "  'iran',\n",
              "  'form',\n",
              "  'new',\n",
              "  'cluster',\n",
              "  'virus'],\n",
              " ['similar',\n",
              "  'sar',\n",
              "  'mer',\n",
              "  'also',\n",
              "  'suspect',\n",
              "  'origin',\n",
              "  'bat',\n",
              "  'case',\n",
              "  'mer',\n",
              "  'dromedari',\n",
              "  'camel',\n",
              "  'intermedi',\n",
              "  'host'],\n",
              " ['bat',\n",
              "  'known',\n",
              "  'harbor',\n",
              "  'coronavirus',\n",
              "  'quit',\n",
              "  'time',\n",
              "  'case',\n",
              "  'avian',\n",
              "  'flu',\n",
              "  'sar',\n",
              "  'mer',\n",
              "  'possibl',\n",
              "  'even',\n",
              "  'hiv',\n",
              "  'increas',\n",
              "  'select',\n",
              "  'ecolog',\n",
              "  'pressur',\n",
              "  'due',\n",
              "  'human',\n",
              "  'activ',\n",
              "  'virus',\n",
              "  'made',\n",
              "  'jump',\n",
              "  'anim',\n",
              "  'man'],\n",
              " ['identif',\n",
              "  'origin',\n",
              "  'host',\n",
              "  'help',\n",
              "  'us',\n",
              "  'contain',\n",
              "  'futur',\n",
              "  'spread',\n",
              "  'well',\n",
              "  'learn',\n",
              "  'mechan',\n",
              "  'transmiss',\n",
              "  'virus']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wynTkVQpAedv"
      },
      "source": [
        "p_ygxvec = np.zeros((numfv2, numlabel)) #posterior distribution, p_ygxvec(x-vector, y) := probability of label y given sentence x-vec#tors\n",
        "\n",
        "for i in range(0, numfv2):\n",
        "    store = np.zeros(numlabel)\n",
        "    for j in lab_val: \n",
        "        temp = 1\n",
        "        for k in corpus2[i]: #access words in i-th sentence\n",
        "            if k in voc:\n",
        "                temp = temp*p_xgy[lab_val[j], voc[k]] #successive product of class-conditional prob.\n",
        "            else:\n",
        "                temp = temp*0.00000001 #edit\n",
        "        store[lab_val[j]] = temp*prior_dis[lab_val[j]] #numerator of Bayes formula\n",
        "            \n",
        "    K = sum(store) #denominator in Bayes formula\n",
        "    \n",
        "    for j in lab_val:\n",
        "        p_ygxvec[i, lab_val[j]] = store[lab_val[j]]/K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVBY70RcAnec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566d6c93-b851-49a8-cd88-f3182ef13320"
      },
      "source": [
        "pred_lab = []\n",
        "for i in range(0, numfv2):\n",
        "    temp = np.argmax(p_ygxvec[i])\n",
        "    for k in lab_val:\n",
        "        if lab_val[k] == temp:\n",
        "            pred_lab.append(k)\n",
        "            break\n",
        "            \n",
        "pred_lab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['science',\n",
              " 'business',\n",
              " 'science',\n",
              " 'science',\n",
              " 'science',\n",
              " 'sports',\n",
              " 'sports',\n",
              " 'sports',\n",
              " 'sports',\n",
              " 'sports',\n",
              " 'business',\n",
              " 'business',\n",
              " 'business',\n",
              " 'business',\n",
              " 'business',\n",
              " 'covid',\n",
              " 'covid',\n",
              " 'covid',\n",
              " 'covid',\n",
              " 'covid']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}